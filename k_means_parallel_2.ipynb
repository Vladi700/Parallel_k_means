{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882cfa1-7ee4-4b44-8f96-7800d8f42947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.cluster import KMeans\n",
    "import numpy as np\n",
    "from dask_ml.metrics import pairwise_distances, pairwise_distances_argmin_min\n",
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "from dask_ml.datasets import make_blobs\n",
    "import pandas as pd\n",
    "from dask.distributed import SSHCluster\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ba1557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPs / hostnames of your three VMs\n",
    "hosts = [\n",
    "    \"10.67.22.199\",\n",
    "    \"10.67.22.199\",   # VM 1  – will run the scheduler *and* a worker\n",
    "    \"10.67.22.138\",   # VM 2  – workers only\n",
    "    \"10.67.22.85\"    # VM 3  – workers only\n",
    "]\n",
    "cluster = SSHCluster(\n",
    "        hosts,\n",
    "        connect_options={\n",
    "            \"username\": \"ungureanu\",       # SSH user on all three VMs\n",
    "            \"password\": getpass.getpass(\"SSH password:\"),\n",
    "            \"known_hosts\": None,\n",
    "        },\n",
    "        remote_python=\"/opt/miniconda3/envs/dask-env/bin/python\", \n",
    "        scheduler_options={\n",
    "            \"port\": 8786,\n",
    "            \"dashboard_address\":\":8787\",\n",
    "        },\n",
    "        worker_options={\n",
    "            \"n_workers\": 1,        \n",
    "            \"nthreads\": 4,      \n",
    "            \"memory_limit\": \"7.8GB\"\n",
    "        },\n",
    "        \n",
    "    )\n",
    "    \n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90922043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.close()\n",
    "#client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe8bf6-617b-441b-8a7c-3711851a1ca5",
   "metadata": {},
   "source": [
    "# original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1fc3bf6-a8c5-4e21-a9c0-df5dabe0b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(X,C):\n",
    "    return pairwise_distances(X, C, metric='sqeuclidean').min(1).sum() \n",
    "    # Compute the sum of distances to the nearest centroid at axis=1\n",
    "    \n",
    "def kmeans_parallel_init_dask(X, k, l):\n",
    "    # Step 1: Randomly select a point from X\n",
    "    n, d = X.shape\n",
    "    idx = np.random.choice(n, size=1)\n",
    "    C = X[idx].compute()  # Collect to memory for use\n",
    "\n",
    "    # Step 2: Compute φX(C)\n",
    "    phi_X_C = phi(X, C).compute() # Compute the sum of distances to the nearest centroid\n",
    "\n",
    "    # Steps 3-6: Repeat O(log φX(C)) times\n",
    "    rounds = int(np.log(phi_X_C))\n",
    "    #print(f\"Begin centroid sampling with number of rounds: {rounds}\")\n",
    "    for _ in range(rounds):\n",
    "        dist_sq = pairwise_distances(X, C, metric='sqeuclidean').min(1)\n",
    "        dist_sum = dist_sq.sum()\n",
    "        p_x = l * dist_sq / dist_sum\n",
    "        samples = da.random.uniform(size=len(p_x), chunks=p_x.chunks) < p_x\n",
    "        sampled_idx = da.where(samples)[0].compute()\n",
    "        \n",
    "        new_C = X[sorted(sampled_idx)].compute() #https://github.com/dask/dask-ml/issues/39 \n",
    "        C = np.vstack((C, new_C))\n",
    "\n",
    "    # Step 7: Compute weights\n",
    "    dist_to_C = pairwise_distances(X, C, metric='euclidean')\n",
    "    closest_C = da.argmin(dist_to_C, axis=1)\n",
    "\n",
    "    weights = np.empty(len(C))\n",
    "    counts = da.bincount(closest_C, minlength=len(C)).compute()\n",
    "    weights[:len(counts)] = counts\n",
    "    \n",
    "    # Normalize weights so that they sum up to the number of centroids\n",
    "    weight_sum = np.sum(weights)\n",
    "    if weight_sum == 0:\n",
    "        raise ValueError(\"Sum of weights is zero, cannot normalize.\")\n",
    "    \n",
    "    weights_normalized = weights / weight_sum\n",
    "    \n",
    "    dask_C = da.from_array(C, chunks=(C.shape[0], C.shape[1])) # here we ensure that the re-clustering occurs on a single-thread\n",
    "\n",
    "    # Step 8: Recluster the weighted points in C into k clusters\n",
    "    #print(\"Begin centroid re-clustering\")\n",
    "    labels, centroids = lloyd_kmeans_plusplus(X=dask_C, weights=weights_normalized,k=k, max_iters=10, tol=1e-8)\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf8444d-6086-4202-a54f-111de5061e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centroids_weighted(X, labels, weights, k):\n",
    "    \"\"\"\n",
    "    Update the centroids by computing the weighted mean of the points assigned to each cluster.\n",
    "    \"\"\"\n",
    "    centroids = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        # Select the points that are assigned to cluster i\n",
    "        cluster_points = X[labels == i]\n",
    "        \n",
    "        # Select the corresponding weights for these points\n",
    "        cluster_weights = weights[labels == i]\n",
    "        \n",
    "        if len(cluster_points) == 0:\n",
    "            # If no points are assigned to this cluster, avoid division by zero\n",
    "            # Continue without updating that centroid\n",
    "            continue\n",
    "\n",
    "        # Compute the weighted mean using dask.array.average\n",
    "        weighted_mean = da.average(cluster_points, axis=0, weights=cluster_weights)\n",
    "        \n",
    "        # Append the computed weighted mean to the centroids list\n",
    "        centroids.append(weighted_mean)\n",
    "    \n",
    "    # Convert centroids list to Dask array\n",
    "    return da.stack(centroids)\n",
    "\n",
    "def kmeans_plusplus_init(X, weights, k):\n",
    "    '''\n",
    "    K-means++ initialization to select k initial centroids from X as a numpy array, keeping C as a NumPy array and weighting by provided weights.\n",
    "    '''\n",
    "    n, d = X.shape\n",
    "    # Step 1: Randomly select the first centroid\n",
    "    idx = np.random.choice(n, size=1)\n",
    "    C = X[idx].compute()\n",
    "\n",
    "    for _ in range(1, k):\n",
    "        # Step 2: Compute distances from each point to the nearest centroid\n",
    "        # C is a NumPy array, X is a Dask array\n",
    "        # Compute the distances from each point to the nearest centroid normalizing by weights\n",
    "        distances = pairwise_distances(X, C, metric='sqeuclidean').min(1) * (weights)\n",
    "        \n",
    "        # Compute the probabilities for choosing each point\n",
    "        probabilities = distances / distances.sum()\n",
    "        \n",
    "        # Sample a new point based on these probabilities\n",
    "        new_idx = np.random.choice(n, size=1, p=probabilities)\n",
    "        new_centroid = X[sorted(new_idx)].compute()\n",
    "        \n",
    "        # Add the new centroid to the list\n",
    "        C = np.vstack((C, new_centroid))\n",
    "    return C\n",
    "\n",
    "def lloyd_kmeans_plusplus(X, weights, k, max_iters=100, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Lloyd's algorithm for k-means clustering using Dask weighting the mean to update centroids.\n",
    "    \"\"\"\n",
    "    centroids = kmeans_plusplus_init(X, weights, k)\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        labels = assign_clusters(X, centroids).compute()\n",
    "        new_centroids = update_centroids_weighted(X, labels, weights=weights, k=k).compute()\n",
    "        \n",
    "        if da.allclose(centroids, new_centroids, atol=tol).compute():\n",
    "            #print(f\"Centroid Lloyd Converged after {i+1} iterations.\")\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "\n",
    "    return labels, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b01796-3c7d-48c0-9159-b80343cf1004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters(X, centroids):\n",
    "    \"\"\"\n",
    "    Assign each point to the nearest centroid using Dask to parallelize the computation.\n",
    "    \"\"\"\n",
    "    return pairwise_distances_argmin_min(X, centroids, metric='sqeuclidean')[0]\n",
    "\n",
    "\n",
    "def update_centroids(X, labels, k):\n",
    "    \"\"\"\n",
    "    Update the centroids by computing the mean of the points assigned to each cluster with Dask.\n",
    "    \"\"\"\n",
    "    centroids = da.stack([X[labels == i].mean(axis=0) for i in range(k)])\n",
    "    return centroids\n",
    "    \n",
    "def kmeans_parallel(X, k, max_iters=100, tol=1e-8, l=2):\n",
    "    centroids = kmeans_parallel_init_dask(X, k, l)\n",
    "    for i in range(max_iters):\n",
    "        labels = assign_clusters(X, centroids)\n",
    "        new_centroids = update_centroids(X, labels, k).compute()\n",
    "\n",
    "        if da.allclose(centroids, new_centroids, atol=tol):\n",
    "            #print(f\"Main KMeans Converged after {i+1} iterations.\")\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return labels, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90fbf33-98e7-4bee-b284-3ba56bfa34cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d401b2-048b-4cd3-b925-7345f9ab276f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e7d4194-89d4-4a98-a3b0-49b987179fd7",
   "metadata": {},
   "source": [
    "# modified version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1499fe7-c166-4bb5-8b26-5a0837ed0ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np\n",
    "from dask_ml.metrics import pairwise_distances, pairwise_distances_argmin_min\n",
    "\n",
    "def total_min_distance(data, centroids):\n",
    "    distances = pairwise_distances(data, centroids, metric='sqeuclidean')\n",
    "    return distances.min(axis=1).sum().compute()\n",
    "\n",
    "def initial_candidate_selection(data, num_clusters, oversampling_factor):\n",
    "    num_points, _ = data.shape\n",
    "    seed_idx = np.random.choice(num_points, size=1)\n",
    "    centroid_pool = data[seed_idx].compute()\n",
    "\n",
    "    init_cost = total_min_distance(data, centroid_pool)\n",
    "    num_rounds = int(np.log(init_cost + 1e-6))  # Avoid log(0)\n",
    "\n",
    "    for _ in range(num_rounds):\n",
    "        dist_sq = pairwise_distances(data, centroid_pool, metric='sqeuclidean').min(axis=1)\n",
    "        prob_dist = (oversampling_factor * dist_sq / dist_sq.sum()).compute()\n",
    "        random_values = da.random.random(size=len(prob_dist), chunks=prob_dist.chunks)\n",
    "        selected_mask = random_values < prob_dist\n",
    "        selected_indices = da.where(selected_mask)[0].compute()\n",
    "        new_centroids = data[sorted(selected_indices)].compute()\n",
    "        centroid_pool = np.vstack([centroid_pool, new_centroids])\n",
    "\n",
    "    return centroid_pool\n",
    "\n",
    "def compute_assignment_weights(data, centroids):\n",
    "    distances = pairwise_distances(data, centroids, metric='euclidean')\n",
    "    closest_indices = da.argmin(distances, axis=1)\n",
    "    counts = da.bincount(closest_indices, minlength=len(centroids)).compute()\n",
    "    weight_sum = counts.sum()\n",
    "    if weight_sum == 0:\n",
    "        return np.ones(len(centroids)) / len(centroids)\n",
    "    cluster_weights = counts / weight_sum\n",
    "    return cluster_weights\n",
    "\n",
    "def kmeans_plus_plus_weighted_init(data, cluster_weights, num_clusters):\n",
    "    num_points, _ = data.shape\n",
    "    seed_idx = np.random.choice(num_points, size=1)\n",
    "    centers = data[seed_idx].compute()\n",
    "\n",
    "    for _ in range(1, num_clusters):\n",
    "        dist_sq = pairwise_distances(data, centers, metric='sqeuclidean').min(axis=1) * cluster_weights\n",
    "        prob = dist_sq / dist_sq.sum()\n",
    "        new_idx = np.random.choice(num_points, size=1, p=prob.compute())\n",
    "        new_center = data[sorted(new_idx)].compute()\n",
    "        centers = np.vstack([centers, new_center])\n",
    "\n",
    "    return centers\n",
    "\n",
    "def assign_to_centroids(data, centers):\n",
    "    cluster_labels, _ = pairwise_distances_argmin_min(data, centers, metric='sqeuclidean')\n",
    "    return cluster_labels\n",
    "\n",
    "def weighted_centroid_update(data, cluster_labels, cluster_weights, num_clusters):\n",
    "    updated = []\n",
    "    for cluster_id in range(num_clusters):\n",
    "        cluster_mask = cluster_labels == cluster_id\n",
    "        cluster_data = data[cluster_mask]\n",
    "        weight_subset = cluster_weights[cluster_mask]\n",
    "        if cluster_data.shape[0] == 0:\n",
    "            continue\n",
    "        cluster_mean = da.average(cluster_data, axis=0, weights=weight_subset)\n",
    "        updated.append(cluster_mean)\n",
    "    return da.stack(updated)\n",
    "\n",
    "def run_lloyds(data, cluster_weights, num_clusters, max_iter=100, tolerance=1e-8):\n",
    "    centroids = kmeans_plus_plus_weighted_init(data, cluster_weights, num_clusters)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        cluster_labels = assign_to_centroids(data, centroids).compute()\n",
    "        new_centroids = weighted_centroid_update(data, cluster_labels, cluster_weights, num_clusters).compute()\n",
    "        if da.allclose(new_centroids, centroids, atol=tolerance).compute():\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    return cluster_labels, centroids\n",
    "\n",
    "def run_distributed_kmeans(data, num_clusters, max_iter=100, tolerance=1e-8, oversample_factor=2):\n",
    "    candidate_centroids = initial_candidate_selection(data, num_clusters, oversample_factor)\n",
    "    cluster_weights = compute_assignment_weights(data, candidate_centroids)\n",
    "    dask_centroids = da.from_array(candidate_centroids, chunks=(candidate_centroids.shape[0], candidate_centroids.shape[1]))\n",
    "    cluster_labels, centroids = run_lloyds(dask_centroids, cluster_weights, num_clusters, max_iter, tolerance)\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        cluster_labels = assign_to_centroids(data, centroids)\n",
    "        new_centroids = da.stack([data[cluster_labels == i].mean(axis=0) for i in range(num_clusters)]).compute()\n",
    "        if da.allclose(centroids, new_centroids, atol=tolerance).compute():\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return cluster_labels, centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1cbf05-33d5-4ce8-87fb-558e89cd7005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cd4b038-9cf1-4da8-b979-fb8a2e443d49",
   "metadata": {},
   "source": [
    "# artificial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4f427a1-5ba6-46be-9424-603ec94e1fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob test for the algorithm performance\n",
    "n_samples = 5000000*2.5\n",
    "n_features = 50\n",
    "centers = 5\n",
    "random_state = 42\n",
    "chunks = (n_samples//23,n_features) \n",
    "synt_data, true_labels = make_blobs(n_samples=n_samples, n_features=n_features, \n",
    "                               centers=centers, chunks=chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e311c922-9a7f-4e0c-a804-bce46c08e916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 4.66 GiB </td>\n",
       "                        <td> 207.32 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (12500000, 50) </td>\n",
       "                        <td> (543478, 50) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 24 chunks in 73 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float64 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"5\" x2=\"25\" y2=\"5\" />\n",
       "  <line x1=\"0\" y1=\"10\" x2=\"25\" y2=\"10\" />\n",
       "  <line x1=\"0\" y1=\"15\" x2=\"25\" y2=\"15\" />\n",
       "  <line x1=\"0\" y1=\"26\" x2=\"25\" y2=\"26\" />\n",
       "  <line x1=\"0\" y1=\"31\" x2=\"25\" y2=\"31\" />\n",
       "  <line x1=\"0\" y1=\"36\" x2=\"25\" y2=\"36\" />\n",
       "  <line x1=\"0\" y1=\"41\" x2=\"25\" y2=\"41\" />\n",
       "  <line x1=\"0\" y1=\"52\" x2=\"25\" y2=\"52\" />\n",
       "  <line x1=\"0\" y1=\"57\" x2=\"25\" y2=\"57\" />\n",
       "  <line x1=\"0\" y1=\"62\" x2=\"25\" y2=\"62\" />\n",
       "  <line x1=\"0\" y1=\"67\" x2=\"25\" y2=\"67\" />\n",
       "  <line x1=\"0\" y1=\"78\" x2=\"25\" y2=\"78\" />\n",
       "  <line x1=\"0\" y1=\"83\" x2=\"25\" y2=\"83\" />\n",
       "  <line x1=\"0\" y1=\"88\" x2=\"25\" y2=\"88\" />\n",
       "  <line x1=\"0\" y1=\"93\" x2=\"25\" y2=\"93\" />\n",
       "  <line x1=\"0\" y1=\"104\" x2=\"25\" y2=\"104\" />\n",
       "  <line x1=\"0\" y1=\"109\" x2=\"25\" y2=\"109\" />\n",
       "  <line x1=\"0\" y1=\"114\" x2=\"25\" y2=\"114\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.41261651458249,0.0 25.41261651458249,120.0 0.0,120.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >50</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">12500000</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<concatenate, shape=(12500000, 50), dtype=float64, chunksize=(543478, 50), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11f0b9b6-4291-4c6f-8bf1-3cd6374218b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 42s\n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data before clustering\n",
    "scaler = StandardScaler(with_mean=True) # for scaling with a sparse matrix\n",
    "%time synt_normalized = scaler.fit_transform(synt_data)\n",
    "del synt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef65885-7bf7-478b-bc80-f656fed5cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import performance_report\n",
    "k = centers  # Number of clusters\n",
    "\n",
    "with performance_report(filename=\"kmean_our.html\"):  \n",
    "    #%time synt_labels, synt_centroids = kmeans_parallel(X=synt_normalized, k=k, l=2)  # original\n",
    "    %time synt_labels, synt_centroids = run_distributed_kmeans(X=synt_normalized, k=k, l=2)  # modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d29cb-fb09-4556-aaea-2cca2771c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "# Display the HTML report in Jupyter\n",
    "IFrame(src=\"kmean_our.html\", width=1280, height=720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e5d3a6-3e36-497f-bff0-4e7009786053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a subset of the first 2 dim of the data \n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "subset = int(synt_normalized.shape[0]/10000)\n",
    "\n",
    "# Kmeans labels plot\n",
    "ax[0].scatter(synt_normalized[:subset, 0], synt_normalized[:subset, 1], c=synt_labels[:subset], cmap='viridis')\n",
    "ax[0].scatter(synt_centroids[:, 0], synt_centroids[:, 1], c='red', marker='x', s=100)\n",
    "ax[0].set_title('K-means clustering with Dask')\n",
    "\n",
    "# True labels plot\n",
    "ax[1].scatter(synt_normalized[:subset, 0], synt_normalized[:subset, 1], c=true_labels[:subset], cmap='viridis')\n",
    "ax[1].scatter(synt_centroids[:, 0], synt_centroids[:, 1], c='red', marker='x', s=100)\n",
    "ax[1].set_title('True labels')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
